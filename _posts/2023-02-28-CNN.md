---
title:  "Convolutional Neural Network"

excerpt: CNN 

toc : true
toc_sticky : true  

use_math: true

categories:
  - study
tags:
  - study
  - DeepLearning
  - CNN
---
# 1. 심층 신경망
인공 신경망이란 인간이 사고하는 과정을 수학적으로 모델링하여 
만든 기계학습 모형을 의미한다.  

사람들은 개발자가 모든 경우의 수를
하나씩 입력하여 프로그램을 만들기보다는\
일련의 예제가 주어졌을 때,
이를 보고 스스로 예측해나가는 능력을 가진 솔루션을 원했다. 

인공 신경망은 사람이 사고하는 과정을 모방하였기 때문에 주어진 데이터를 학습하면서\
신경망 모형의 예측이 얼마나 정답에 가까운지를 계속 확인하고 정답과의 오차를 줄여나간다.

이러한 과정을 반복하면 모형은 예를 들어, 스스로 Regression(회귀), Classification(분류)등과 같은 
문제를 풀 수 있게 된다.

 우리가 사용할 수 있는  "**심층 신경망(Deep Neural Network)**"
모형은 다음과 같다. 

![mlp](https://github.com/Sodychoe/sodychoe.github.io/blob/main/assets/images/%20study/CNN/mlp.png?raw=true){: .align-center}

<div style="text-align: center;">Source : [3]</div>

위의 그림은 여러 은닉층(Hidden Layer)을 가지고 있는 심층 신경망이며 이전 층의 입력이
다음 층의 모든 유닛에게 전파된다는 점에서 완전 연결(Fully Connected) 되어있다고 한다.

모델링한 식은 간단하다. 인간의 뉴런이 특정 역치이상의 자극을 받을 때 활성화되는 것 처럼, \
데이터의 입력을 선형 함수인 행렬의 곱으로 표현하고, 뉴런이 활성화되는 여부를 비선형 함수로
표현한 것이다.

 입력 행렬 $X$, 가중치 행렬 W , 편향 벡터 $b$,  활성화 함수가 $\sigma(\cdot)$ 일 때,
 각 은닉층을 통과한 값은 $\sigma(W^tX+b)$ 가 된다. 

그럼 마지막 Hidden Layer 를 통과한 출력 값이 실제 정답과 얼마나 비슷한 지를
수치적으로 나타내는 볼록한 형태의 손실 함수(Loss Function) $J(\cdot)$ 를 정의하고
이를 가중치 행렬에 대해 편미분하는 $\frac{\partial J}{\partial W}$   계산을 통해
역전파(Back Propagation, chain rule 이라고도 한다.)를 수행하여
손실 함수를 최소로 하는 **최적의 파라미터, 가중치 행렬**을 학습하는 것이 
심층 신경망의 동작 과정이라고 할 수 있다.


# 2. 심층 신경망 모형의 문제점
그런데 인간이 이미지 데이터를 인식하는 경우에도 이런 모형이
우리의 사고과정을 잘 모방한다고 할 수 있을까?

사실 심층 신경망 모형에는 몇 가지 단점들이 존재한다.

1. 오버피팅(Overfitting) 에 취약하다.
- 오버피팅은 모형이 학습을 위해서 입력된 데이터에만 과도하게 의존하는 것을 말한다.
- 이는 모형의 복잡도(파라미터의 개수) 때문에 일어나는 현상이다.
- 모형에 복잡도에 비해서 간단한 난이도의 데이터는 모형의 일반화 능력을 감소시키며 이것이 오버피팅된 상태이다. 

1. 불필요한 계산량이 존재한다.
- 위의 심층 신경망은 완전 연결되어있다고 하였다. 그런데 꼭 완전 연결을 해야 할까?
- 이미지 분류문제를 풀기 위해서 모형은 이미지의 전체적인 특징이 아닌 일부 특징에만 집중해야 할 수 있다.
- 그런데 완전 연결된 신경망은 모든 은닉층에 파라미터를 전파하기 때문에 모형이 처리하지 않아도 될 내용까지 학습해버린다. 

1. 이미지가 가진 공간적인 정보를 보존하지 못한다.
-  2차원 이미지를 심층 신경망에 입력하기 위해서는 평탄화(Flatten) 작업이 선행된다.
-  예를 들어 28x28 크기의 2차원 이미지를 784x1 의 텐서로 형태를 변환하여 입력하는 것이다.
-  이 과정에서 특정 픽셀의 위치, 특정 픽셀들간의 멀고 가까운 정보가 손실될 수 있다.

[//]: # (![sparseconnectivity]&#40;https://github.com/Sodychoe/sodychoe.github.io/blob/main/assets/images/%20study/CNN/sparseconn.png?raw=true&#41;{: .align-center})

[//]: # (<div style="text-align: center;">Source : [2]</div>)





# 3. CNN 의 개념

## 3.1 Convolution

합성곱 신경망(Convolutional Neural Network)은 합성곱, 콘볼루션 연산을 도입하여
앞의 문제들을 해결하고, 이미지 데이터에 대한 문제를 더 효율적으로 풀 수 있다.
이름부터가 합성곱 신경망인 만큼, 핵심이 되는 것이 바로 합성곱 연산이다

합성곱 연산의 수학적 정의는 다음과 같다.

$$f*g(t)=\int_{-\infty}^{\infty}f(\tau)g(t-\tau)d\tau$$

이 연산은 교환, 분배, 결합법칙이 성립힌다. 

합성곱 연산에서는 보통 앞의 함수 f 를 보통 입력(Input) 이라고 하고, 뒤의 함수 g 를 핵(kernel)
또는 필터(filter) 이라고 한다. CNN 관련 자료에서는 두 용어 모두 많이 사용된다. 이 글에서는
핵(Kernel) 이라는 용어를 사용하겠다. 


시간은 연속적이기 때문에 합성곱 연산이 적분기호로 써져 있지만 특정 간격으로 핵 g를 움직인다고 이산적으로 생각할 수도 있다. 이미지가 입력되면 핵이 주어진 이미지를 특정 간격만큼 움직여서 합성곱 연산을 수행한다.

![conv](https://github.com/Sodychoe/sodychoe.github.io/blob/main/assets/images/%20study/CNN/conv.png?raw=true){: .align-center}

<div style="text-align: center;">Source : [2]</div>

[2] 는 Ian Goodfellow 의 [Deep Learning] 책에 있는 합성곱 연산을 설명하는 그림이다. 2차원 형태의 이미지가 입력되었을 때,  핵(kernel)과 이미지가 합성곱 연산을 수행하고 있다. 그 모양은 핵이 특정 간격만큼 움직이면서 각 픽셀과 내적(Inner Product)를 수행하는 것이다. 

이 연산을 수행할 때, 신경망에서 우리가 학습시켜야 하는 파라미터 중 하나는 핵(kernel)이 된다.
심층 신경망의 가중치 텐서라고 생각하면 되는데, 합성곱 연산을 수행하면서
1. 여러 픽셀이 같은 가중치를 공유하여 모형의 복잡도가 감소함(Weight Sharing)
2. 유닛이 가지는 가중치가 Local 하기 때문에 신경망이 이미지의 이동에 강건함(Translation Invariant)
3. 이미지 픽셀 간의 공간적인 상관관계가 유지됨(Spatial Correlation)


이라는 장점을 얻을 수 있다.

이미지에서의 합성곱 연산이, 합성곱 연산이 갖는 이론적인 배경을 모두 설명하는 예시가 되지는 않는다.
자세한 것은 나중에 작성할 GCN 관련 글에서 구체적인 합성곱의 쓰임을 살펴볼 것이다.

## 3.2 Pooling

## 3.3 Forward

## 3.4 Backward

## 3.5 Summary

![convstructure](https://raw.githubusercontent.com/Sodychoe/sodychoe.github.io/main/assets/images/%20study/CNN/cnnstructure.webp){: .align-center}

<div style="text-align: center;">Source : [5]</div>


# 4. References

1. [모두를 위한 딥러닝 시즌 2 : Pytorch](https://deeplearningzerotoall.github.io/season2/)
2. [Deep Learning - Ian Goodfellow and Yoshua Bengio and Aaron Courville, 2016](https://www.deeplearningbook.org/)
3. [Neural Networks and Deep Learning -Michael Nielsen , 2019 ](http://neuralnetworksanddeeplearning.com/index.html) 
4. [CNN 역전파를 이해하는 가장 쉬운 방법](https://metamath1.github.io/cnn/index.html)
3. [A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)

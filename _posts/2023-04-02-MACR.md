---
title:  Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System(KDD 2021)

excerpt: Causal Inference, Recommendation System, KDD 

toc : true
toc_sticky : true  

use_math: true

categories:
  - papers
tags:
  - papers
  - MACR
  - causal
  - recsys
---

# 0. Abstract 
추천 시스템의 일반적인 목표는 유저들에게 단순히 인기 있는 아이템을 제안하는 것이 아니라, 개개인에 맞는 아이템은 제안하는 것이다. 보통 일반적인 훈련 패러다임은 추천 시스템 모델이 손실 함수를 가지고
유저의 행동 데이터를 복원하는 방향으로 모델을 적합시킨다. 그런데 이는 모형을 인기있는 아이템들에 치우치도록 편향되도록 만든다. 그 결과 모델은 안 좋은 결과들을 야기하는데 그 예로 마태 효과(인기 많은 아이템들이 더 많이 추천되고 그 결과 더 인기가 많아지는 현상)가 있다. 선행 연구는 훈련 과정에서 인기 있는 아이템들의 영향을 감소시키고 꼬리가 긴 아이템들의 영향을 증가시키는 Inverse Probability Weighting(IPW) 방법론을 활용하여 이 문제를 다뤘다. 여기서 꼬리가 긴 아이템들은(long-tail item) 이 논문에서는 추천을 적게 받는 다수의 아이템들을 의미한다. 그렇지만 IPW 방법은 가중치를 정하는 방법에 따라 매우
민감하게 효과가 변화하며, 모델 튜닝이 어려운 단점이 있다. 이 논문에서는 먼저 인기 편향(Popularity Bias)을 인과 효과적 측면이라는 참신하면서도 본질적인 관점에서 살펴본다. 논문에서는 인기 편향을 
아이템 노드(node) 에서 순위 점수에 영향을 미치는 직접적인 효과임을 확인한다. 이 떄문에 아이템의 본질적인 특성은 그 아이템을 높은 순위 점수에 잘못 할당하는 원인이 된다. 이러한 인기 편향을 제거하기 위해
다음과 같은 반사실적 질문에 답하는 것이 필수적이다, "추천 시스템 모형이 아이템의 특성만을 사용한다면 순위 점수는 어떻게 될까?" 결론적으로, 추천 과정에서의 중요한 원인-결과 관계들을 묘사하는 인과 그래프를
구성할 수 있다. 훈련 과정에서, 각 원인의 기여도를 확인하기 위한 멀티 테스크 러닝을 수행하였다. 평가 과정에서, 아이템 인기의 효과를 제거하기 위한 반사실적 추론을 수행하였다. 주목할 만한 사실은 이 논문에서
제안하는 방법이 광범위한 모델들에 적용 가능한 추천 시스템의 학습 과정을 개선한다는 것이다. 이는 기존 방법론에서 쉽게 구현할 수 있다. 이것을 협업 필터리의 전통적인 모델인 MF 와 SOTA 를 달성한 LightGCN 에 적용하여 확인하였다. 5개의 실제 데이터셋에서 진행된 실험은 논문에저 제안하는 방법론의 효과적임을 입증한다. 
 


# 1. Introduction

![Popularity Bias](https://github.com/Sodychoe/sodychoe.github.io/blob/main/assets/images/papers/macr/macr-bias.png?raw=true)
<div style="text-align: center;">Figure : 1</div>



# 2. Problem Definition

# 3. Methodology

## 3.1 Preliminaries

## 3.2 Causal Look at Recommendation

## 3.3 Model-Agnostic Counterfactual Reasoning

## 3.4 Rationality of the Debiased Inference

## 3.5 Discussion

# 4. Experiments 

## 4.1 Experiment Settings

## 4.2 Results

## 4.3 Case Study

# 5. Related Work

## 5.1 Popularity Bias in Recommendation

## 5.2 Causal Inference in Recommendation

# 6. Conclusion and Future Work 